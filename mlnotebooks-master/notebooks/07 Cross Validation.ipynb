{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we showcase examples of using cross validation functions, contained within the kdb+/q machine learning toolkit (ML-toolkit).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "To run the below notebook, ensure that dependencies specified in <b>requirements.txt</b> have been correctly installed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML-Toolkit contains general use utilities, an implementation of the FRESH (Feature Extraction based on Scalable Hypothesis tests) algorithm and cross validation functions. The primary purpose of these libraries are to provide kdb+/q users with access to commonly-used ML functions for preprocessing data, extracting features and scoring results.\n",
    "\n",
    "The toolkit is used throughout this notebook and can be loaded using the below syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\l ml/ml.q\n",
    ".ml.loadfile`:init.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a model validation technique that is used to assess how well the results produced by a model generalise to independent datasets. The aim of performing this technique is to train the algorithm using a variety of validation datasets in order to limit future problems with prediction, such as overfitting or underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) is a set of 569 samples of fine needle aspirate (FNA) of breast mass. Each sample contains features describing characteristics of the cell nuclei, along with a classification of the sample as either benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "In the cells below, we load in the breast cancer data as well as graphics functions required for this notebook. The diagnosis column is removed from the dataset and used as the target vector as this is the feature we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\c 15 100\n",
    "\\l ../utils/graphics.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean co..\n",
      "-------------------------------------------------------------------------------------------------..\n",
      "842302      17.99       10.38        122.8          1001      0.1184          0.2776           0...\n",
      "842517      20.57       17.77        132.9          1326      0.08474         0.07864          0...\n",
      "8.43009e+07 19.69       21.25        130            1203      0.1096          0.1599           0...\n",
      "8.43483e+07 11.42       20.38        77.58          386.1     0.1425          0.2839           0...\n",
      "8.43584e+07 20.29       14.34        135.1          1297      0.1003          0.1328           0...\n",
      "\n",
      "Shape of data is: 569 x 31\n",
      "\n",
      "`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`B`B`B`M`M`M`M`M`M`M`M`M`M`M`M`M`M`M`B`M`M`M`M`M`M`M`M`B`M`..\n"
     ]
    }
   ],
   "source": [
    "targets:select diagnosis from data:(\"FS\",30#\"F\";(),\",\")0:`:../data/data.csv\n",
    "show 5#data:delete diagnosis from data\n",
    "-1\"\\nShape of data is: \",(\" x \"sv string .ml.shape data),\"\\n\";\n",
    "show targets`diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target values\n",
    "\n",
    "One hot encoding is used on the target data to convert symbols into a numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0..\n"
     ]
    }
   ],
   "source": [
    "show targets:exec diagnosis_M from .ml.onehot[targets;cols targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "In the cells below, polynomial features are produced from the original data table to allow for interactions between terms in the system. This allows us to study both individual and combined features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "17.99       10.38        122.8          1001      0.1184          0.2776           0.3001        ..\n",
       "20.57       17.77        132.9          1326      0.08474         0.07864          0.0869        ..\n",
       "19.69       21.25        130            1203      0.1096          0.1599           0.1974        ..\n",
       "11.42       20.38        77.58          386.1     0.1425          0.2839           0.2414        ..\n",
       "20.29       14.34        135.1          1297      0.1003          0.1328           0.198         ..\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ target classifications should be agnostic of the id column\n",
    "5#table:(cols[data]except`id)#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "17.99       10.38        122.8          1001      0.1184          0.2776           0.3001        ..\n",
       "20.57       17.77        132.9          1326      0.08474         0.07864          0.0869        ..\n",
       "19.69       21.25        130            1203      0.1096          0.1599           0.1974        ..\n",
       "11.42       20.38        77.58          386.1     0.1425          0.2839           0.2414        ..\n",
       "20.29       14.34        135.1          1297      0.1003          0.1328           0.198         ..\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ add second order polynomial features to the table \n",
    "5#table:table^.ml.polytab[table;2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "radius_mean texture_mean perimeter_mean area_mean smoothness_mean compactness_mean concavity_mean..\n",
       "-------------------------------------------------------------------------------------------------..\n",
       "0.5210374   0.0226581    0.5459885      0.3637328 0.5937528       0.7920373        0.7031396     ..\n",
       "0.6431445   0.2725736    0.6157833      0.5015907 0.2898799       0.181768         0.2036082     ..\n",
       "0.6014956   0.3902604    0.5957432      0.4494168 0.5143089       0.4310165        0.4625117     ..\n",
       "0.2100904   0.3608387    0.2335015      0.1029056 0.8113208       0.8113613        0.5656045     ..\n",
       "0.6298926   0.1565776    0.6309861      0.4892895 0.4303512       0.3478928        0.4639175     ..\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/ complete min-max scaling of the dataset to avoid biases due to orders of magnitude in the data\n",
    "5#table:.ml.minmaxscaler table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain| +`radius_mean`texture_mean`perimeter_mean`area_mean`smoothness_mean`compactness_mean`conc..\n",
      "ytrain| 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0..\n",
      "xtest | +`radius_mean`texture_mean`perimeter_mean`area_mean`smoothness_mean`compactness_mean`conc..\n",
      "ytest | 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0..\n"
     ]
    }
   ],
   "source": [
    "/ complete a train-test-split on the data - below 20% of data is used in the test set\n",
    "show tts:.ml.traintestsplit[table;targets;.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "Below a Random Forest Classifier model is initialized in order to classify tumours as malignant or benign. We can perform consistency checks on this model by performing cross validation techniques on the training data. In the first cell, cross-validation is applied in 5 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k:5  / number of folds\n",
    "n:1  / number of repetitions\n",
    "\n",
    "xtrain:flip value flip tts`xtrain\n",
    "ytrain:tts`ytrain\n",
    "\n",
    "/ function with algorithm\n",
    "a:{.p.import[`sklearn.ensemble][`:RandomForestClassifier]}\n",
    "\n",
    "/ scoring function which takes a function, parameters to apply to that function and data as arguments\n",
    "score_func:.ml.xv.fitscore[a][`n_estimators pykw 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Scores:\n",
      "----------------------------------------------------------------------------\n",
      "Sequential split indices with basic k-fold cross validation: 0.9736264\n",
      "Random split indices with basic k-fold cross validation: 0.9802198\n",
      "Stratified split indices with basic k-fold cross validation: 0.9780692\n"
     ]
    }
   ],
   "source": [
    "/ split data into k-folds and train/validate the model\n",
    "s1:.ml.xv.kfsplit[k;n;xtrain;ytrain;score_func]  / sequentially split\n",
    "s2:.ml.xv.kfshuff[k;n;xtrain;ytrain;score_func]  / randomized split\n",
    "s3:.ml.xv.kfstrat[k;n;xtrain;ytrain;score_func]  / stratified split\n",
    "\n",
    "-1\"Average Model Scores:\";\n",
    "-1\"----------------------------------------------------------------------------\";\n",
    "-1\"Sequential split indices with basic k-fold cross validation: \",string avg s1;\n",
    "-1\"Random split indices with basic k-fold cross validation: \",string avg s2;\n",
    "-1\"Stratified split indices with basic k-fold cross validation: \",string avg s3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use repeated forms of cross validation, such as monte-carlo or repeated k-fold cross validation. These methods have the benefit of allowing a user to evaluate the consistency and robustness of the models produced. Below 5 folds are again used, this time with 5 repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model Scores:\n",
      "----------------------------------------------------------------------------\n",
      "Monte-Carlo cross validation with 5 repetitions and training size of 80%: 0.9648352\n",
      "Repeated stratified cross validation, 5 fold, 5 repetitions: 0.9731868\n",
      "Repeated sequential cross validation, 5 fold, 5 repetitions: 0.9731868\n"
     ]
    }
   ],
   "source": [
    "p:.2  / percentage of data in validation set\n",
    "n: 5  / number of repetitions\n",
    "\n",
    "r1:.ml.xv.mcsplit[p;n;xtrain;ytrain;score_func]\n",
    "r2:.ml.xv.kfshuff[k;n;xtrain;ytrain;score_func]\n",
    "r3:.ml.xv.kfsplit[k;n;xtrain;ytrain;score_func]\n",
    "\n",
    "-1\"Average Model Scores:\";\n",
    "-1\"----------------------------------------------------------------------------\";\n",
    "-1\"Monte-Carlo cross validation with 5 repetitions and training size of 80%: \",string avg r1;\n",
    "-1\"Repeated stratified cross validation, 5 fold, 5 repetitions: \",string avg r2;\n",
    "-1\"Repeated sequential cross validation, 5 fold, 5 repetitions: \",string avg r3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "An alternative method is to perform a grid search over possible sets of hyperparameters in order to find the optimal model for the dataset. Grid search can be completed on the training data to find the best parameters which are then be applied to the model. Predictions are then made and scored using the unseen testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "/ new scoring function\n",
    "sf:.ml.xv.fitscore[a]\n",
    "\n",
    "/ dictionary of parameters\n",
    "pd:`n_estimators`criterion`max_depth!(10 50 100 500;`gini`entropy;2 5 10 20 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the grid search function below, the final argument is a float value denoting the size of the holdout set used in a fitted gridsearch where the best model is fit to holdout data. If 0 is used (shown below) the function will return scores for each fold for the given hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid search: hyperparameters and resulting score from each fold:\n",
      "\n",
      "n_estimators criterion max_depth|                                                                ..\n",
      "--------------------------------| ---------------------------------------------------------------..\n",
      "10           gini      2        | 0.9450549 0.9450549 0.9340659 0.956044  0.967033  0.9340659 0.9..\n",
      "10           gini      5        | 0.978022  0.978022  0.956044  0.956044  0.956044  0.967033  0.9..\n",
      "10           gini      10       | 0.967033  0.989011  0.978022  0.956044  0.956044  0.9450549 0.9..\n",
      "10           gini      20       | 0.9450549 0.978022  0.9340659 0.9450549 0.956044  0.956044  0.9..\n",
      "10           gini      30       | 0.9450549 0.978022  0.956044  0.956044  0.9340659 0.967033  0.9..\n",
      "10           entropy   2        | 0.9340659 0.956044  0.978022  0.978022  0.9230769 0.967033  0.9..\n",
      "10           entropy   5        | 0.967033  0.989011  0.989011  0.978022  0.956044  0.9450549 0.9..\n",
      "10           entropy   10       | 0.956044  0.956044  0.967033  0.967033  0.956044  0.956044  0.9..\n",
      "10           entropy   20       | 0.978022  0.9450549 0.967033  0.9450549 0.967033  0.967033  0.9..\n",
      "10           entropy   30       | 0.978022  0.978022  0.956044  0.978022  0.967033  0.978022  0.9..\n",
      "50           gini      2        | 0.9450549 0.978022  0.978022  0.967033  0.978022  0.956044  0.9..\n",
      "50           gini      5        | 0.956044  0.978022  0.9450549 0.978022  0.967033  0.956044  0.9..\n",
      "..\n"
     ]
    }
   ],
   "source": [
    "-1\"Grid search: hyperparameters and resulting score from each fold:\\n\";\n",
    "show gr:.ml.gs.kfsplit[k;n;xtrain;ytrain;sf;pd;0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit this best model on our training set and test how well it generalises to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for the 'best model' on the testing set was: 0.9736842\n"
     ]
    }
   ],
   "source": [
    "bstmdl:.p.import[`sklearn.ensemble][`:RandomForestClassifier][pykwargs first where a=max a:avg each gr]\n",
    "bstmdl[`:fit][xtrain;ytrain];\n",
    "br:bstmdl[`:score][flip value flip tts`xtest;tts`ytest]`\n",
    "-1\"Score for the 'best model' on the testing set was: \",string br;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the previous two cells can be compressed within a fitted grid search procedure. As explained above, this is done by using a float for the final parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`n_estimators`criterion`max_depth!(50;`entropy;20)\n",
       "0.9912281\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2#.ml.gs.kfsplit[k;n;flip value flip table;targets;sf;pd;.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search function can also be passed a negative value for the final parameter. This means that data will be shuffled prior to designation of the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "`n_estimators`criterion`max_depth!(50;`entropy;30)\n",
       "0.9649123\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-2#.ml.gs.kfsplit[k;n;flip value flip table;targets;sf;pd;-.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Cross validation is a useful technique to determine how well a model will generalize to new data. It is possible to carry out cross validation in a number of ways depending on the chosen dataset. Above we displayed how to perform cross validation using a range methods, which split data in a sequential, randomized or stratified manner, as well as using monte-carlo methods.\n",
    "\n",
    "It is clear that if the aim is to trial a range of hyperparameters on a model, then grid search is a robust way to test the chosen range of parameters and return the ones which allow the model to generalize best to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
